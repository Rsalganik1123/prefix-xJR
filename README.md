# Overview 
This is the unofficial repository for the prefix-xJR axiom project. Below, we provide the documentation for reproducing and extending our results. 

# Libraries 
Please see ```requirements.txt``` for a list of all the packages used for obtaining our empirical results. 
Please explicitly make sure that you have an ILP solver installed: ```pip install "cvxpy[CBC,CVXOPT,GLOP,GLPK,GUROBI,MOSEK,PDLP,SCIP,XPRESS]"``` 

# Datasets 

### MovieLens 
You can access the dataset at https://grouplens.org/datasets/movielens/1m/ . 
Please make sure it follows the file structure: 
```
ml-1m/
├── movies.dat     # Movie metadata (title, genres)
├── ratings.dat    # User–movie ratings
├── users.dat      # User demographic information
```
### Goodreads 
You can access the full dataset at https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home. 
Please note that, in order to make our empirical evaluations feasible, we significantly shrunk the dataset by randomly sampling user profiles and item indices. You can access our sub-sampled version of the dataset. Note: For the code used to sub-sample the dataset, please see: . 
```
goodreads/
├── goodreads_sample.csv     # Interaction pandas  
```

Please note that for both datasets we provide ```*.yaml``` files that can be used to modify their internal keys and storage locations. Practitioners who wish to augment our results with an additional dataset should create a folder with their dataset and follow the ```*.yaml``` formatting structure to make their loads compatible with our pipeline. 

# Experimental Pipeline 
We provide the full experimental pipeline for reproducing our results on both datasets. Intuitively, we name the files such that they follow a lexiographic ordering (e.g. 01 -- data transformations, 02 -- rank aggregation, 03 -- evaluation, 04 -- result gathering). Please note that for efficiency we design parallelized versions of each algorithm. 

For ease, we present a complete run script under ```run_parallel_exp.sh``` which can be used to launch our exact experimental pipeline. This should be launched using the following arguments: ```sh run_parallel_exp.sh --dataset ml-1m``` (or ```--dataset goodreads```).

Additionally, we also allow practitioners to run individual portions of our experimental pipeline. The code is structured as follows: 
* ```01a_recsys_pipeline.py``` : loads the dataset, converts it to a dataframe, transforms the partial preferences into a complete preference profile stores the model checkpoint and results 
* ```01b_generate_groups.py``` : loads the dataset and generates both item-level and user-level groups for downstream fairness evaluation. 
* ```02_generate_agg_with_sampling_parallel.py```: loads the preference profiles generated by the recommender system, subsamples 100 experiments of truncated preference profiles, compiles aggregated consensus rankings using all the methods presented in our paper (both benchmarks and our proposed methodologies)
* ```03a_kendall_tau_calc_parallel.py```: loads the rank aggregation outputs and evaluates their kendall tau performance 
* ```03b_axiom_satisfaction_calc_parallel.py```: loads the rank aggregation outputs and evaluates their satisfaction of the three prefix-xJR axioms 
* ```03c_diversity_calc_parallel.py```: loads the rank aggregation outputs and evaluates their distribution over the different item-level groups
* ```04_gather_results.py```: gathers all the results from the three evaluation protocols, prints, and saves them

 
